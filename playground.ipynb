{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/napatswift/mtr-en2th/blob/main/playground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4ftRf3TbxJp",
        "outputId": "126229ab-ee86-405e-f19e-35947e4f81ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "f6UIktQ7cTLE"
      },
      "outputs": [],
      "source": [
        "# configuration\n",
        "\n",
        "train_sentence_piece = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QxGUWcnKLudH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sentencepiece as spm # tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqxqUbzMdLdK"
      },
      "source": [
        "ใช้ colab authentication api เพื่อจะดึงข้อมูลจากชีต"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ylnp0--Fau4z"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PsrAOK14Zi0o"
      },
      "outputs": [],
      "source": [
        "import gspread\n",
        "from google.auth import default\n",
        "\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "j3YBFk8GZ3qg"
      },
      "outputs": [],
      "source": [
        "spread_sheet = gc.open_by_key('1wd-mlGq_XETPD6szP-Q9XJ9xl8SVOMhmakpQLEkkSrg')\n",
        "sheet = spread_sheet.worksheets()[0] # เลือกแผ่นแรก\n",
        "dataset_df = pd.DataFrame(sheet.get_all_records())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lLTINU_pbZD2"
      },
      "outputs": [],
      "source": [
        "if train_sentence_piece:\n",
        "  !touch lines.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zFLFVMw8bL7o"
      },
      "outputs": [],
      "source": [
        "# create dummy function to prevent polluting global namespace\n",
        "def _f():\n",
        "  sentence_lines = dataset_df[(dataset_df.Thai != '<song title>') &\n",
        "           (~(dataset_df.Thai == '')) &\n",
        "           (~(dataset_df.English == ''))][['Thai', 'English']].values\n",
        "  total_line_count = 0\n",
        "  with open('lines.txt', 'w') as f:\n",
        "    string_lines = [str(m) for l in sentence_lines for m in l]\n",
        "    for l in string_lines:\n",
        "      f.write(l)\n",
        "      f.write('\\n')\n",
        "      total_line_count += 1\n",
        "    print('total_line_count', total_line_count)\n",
        "\n",
        "if train_sentence_piece:\n",
        "  _f()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f3yjf51c2ks"
      },
      "source": [
        "create folder to store sentence piece model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aw0QxiZIIr2M"
      },
      "outputs": [],
      "source": [
        "!if [[ ! -e model ]];\\\n",
        "  then mkdir model;\\\n",
        "fi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "IvDFVbJpNgLv"
      },
      "outputs": [],
      "source": [
        "if train_sentence_piece:\n",
        "  spm.SentencePieceTrainer.train(\n",
        "        input='lines.txt',\n",
        "        model_prefix='model/m',\n",
        "        vocab_size=4_800,\n",
        "        pad_id=3)\n",
        "  model = spm.SentencePieceProcessor(model_file='model/m.model')\n",
        "  print(*model.encode(\"\"\"ชีวิตของผมสั้นไป แต่ผมก็มีชีวิตที่ดีมาก ๆ นะ\n",
        "My life was kinda short, but I got so many blessings\n",
        "ผมมีความสุขมากที่คุณเคยเป็นคนรักของผม มันแค่ห่วยมากตรงที่เราต้องจบกันแบบนี้\n",
        "Happy you were mine, it sucks that it’s all ending\"\"\".split('\\n'), out_type=str), sep='\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-W0hHjqOgrZR"
      },
      "source": [
        "โหลด sentencepiece จาก github"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd7FWkVzgqv3",
        "outputId": "753385e4-ef93-4a3f-a969-08fa990984dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'mtr-en2th'...\n",
            "remote: Enumerating objects: 13, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 13 (delta 2), reused 13 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (13/13), 281.55 KiB | 1.26 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/napatswift/mtr-en2th.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "xryDaP5hhdw9"
      },
      "outputs": [],
      "source": [
        "class TokenizerWraperForSP:\n",
        "  def __init__(self, spm):\n",
        "    self.spm = spm\n",
        "    self.vocab_size = spm.vocab_size()\n",
        "    self.pad_id = spm.pad_id()\n",
        "  \n",
        "  def tokenize(self, text_or_list, **kargs):\n",
        "    return self.spm.encode(text_or_list, **kargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "fKTPZIjCh94u"
      },
      "outputs": [],
      "source": [
        "tokenizer = TokenizerWraperForSP(\n",
        "    spm=spm.SentencePieceProcessor(model_file='mtr-en2th/spmodel/m48.model')\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSkXbQ8edpK-"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5SoF48UePx-"
      },
      "source": [
        "เลือกแถวที่จับคู่แล้ว `dataset_df['Matched?'] == 'TRUE'` และ ไม่ใช่แถวที่เป็นชื่อเพลง `dataset_df.Thai != '<song title>'`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "4IQAZFrQi1C9"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "JqlwySABOE_G"
      },
      "outputs": [],
      "source": [
        "en_th_ds = dataset_df[(dataset_df['Matched?'] == 'TRUE') & (dataset_df.Thai != '<song title>')][['English','Thai',]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "H7ApcwrsRmRG"
      },
      "outputs": [],
      "source": [
        "en_th_ds.loc[:, 'en2id'] = tokenizer.tokenize(en_th_ds.English.astype(str).to_list(), add_bos=True, add_eos=True,)\n",
        "en_th_ds.loc[:, 'th2id'] = tokenizer.tokenize(en_th_ds.Thai.astype(str).to_list(), add_bos=True, add_eos=True,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "6uwIgfuLet8I"
      },
      "outputs": [],
      "source": [
        "seq_max_len = en_th_ds.en2id.apply(len).max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "M6bii0qce4S5"
      },
      "outputs": [],
      "source": [
        "sequences = en_th_ds.en2id.apply(lambda seq: seq + [tokenizer.pad_id]*(seq_max_len-len(seq))).to_list()\n",
        "x_train = np.array(sequences)\n",
        "y_train = en_th_ds.th2id.apply(lambda seq: seq + [tokenizer.pad_id]*(seq_max_len-len(seq))).to_list()\n",
        "# y_train = np.array([tf.keras.utils.to_categorical(y, num_classes=tokenizer.vocab_size).sum(0) for y in y_train])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAOVAgZWiy1e"
      },
      "source": [
        "# Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "yooqdaytGbLp"
      },
      "outputs": [],
      "source": [
        "def gen_data(xs, ys):\n",
        "  for x, y in zip(xs, ys):\n",
        "    for yi in y:\n",
        "      yield x, yi\n",
        "\n",
        "ds = tf.data.Dataset.from_generator(\n",
        "    lambda: gen_data(x_train, y_train),\n",
        "    output_types=(tf.int16, tf.int16))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJcTAijwH6oB",
        "outputId": "510fe75d-38d6-47dc-8074-93ee8d78d5ce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(54,), dtype=int16, numpy=\n",
              " array([   1,  471,    7,    6,   37,   11, 2084,    2,    3,    3,    3,\n",
              "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
              "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
              "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
              "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3],\n",
              "       dtype=int16)>, <tf.Tensor: shape=(), dtype=int16, numpy=1>)"
            ]
          },
          "execution_count": 132,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next(iter(ds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "qIL_o-ryi7hQ"
      },
      "outputs": [],
      "source": [
        "input = tf.keras.layers.Input((54,))\n",
        "y = tf.keras.layers.Embedding(tokenizer.spm.vocab_size(), 512,)(input)\n",
        "y = tf.keras.layers.Dropout(0.5)(y)\n",
        "y = tf.keras.layers.Flatten()(y)\n",
        "y = tf.keras.layers.Dropout(0.5)(y)\n",
        "y = tf.keras.layers.Dense(1024, 'relu')(y)\n",
        "y = tf.keras.layers.Dropout(0.5)(y)\n",
        "y = tf.keras.layers.Dense(tokenizer.spm.vocab_size(), 'softmax')(y)\n",
        "model = tf.keras.models.Model(inputs=input,outputs=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "CI0SYrcL_xno"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.MeanAbsoluteError(),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuoP7fYICf2_",
        "outputId": "08abce5b-1fe5-4363-a48c-36dc11ff26fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9/9 [==============================] - 7s 648ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.0110 - val_accuracy: 1.0000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4ffcce2220>"
            ]
          },
          "execution_count": 115,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(x_train[:100], y_train[:100],\n",
        "          batch_size=12,\n",
        "          validation_data=(x_train[-100:], y_train[-100:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjP4007ODbDO",
        "outputId": "7c08bfe2-af25-47e7-faef-13c335a04429"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 80ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[1.1019687e-32, 2.9411009e-17, 1.5045997e-16, ..., 6.4621457e-33,\n",
              "        5.1085639e-34, 1.0616384e-32]], dtype=float32)"
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.predict(x_train[:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tziDdXCjFlM8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([['oh, even the white kids!', 'โอ้ แม้แต่เจ้าเด็กผิวขาวก็ด้วย'],\n",
              "       ['lay on the floor of your living room',\n",
              "        'มาลงจอดบนพื้นห้องนั่งเล่นของเธอ'],\n",
              "       ['and i’m pretendin’ you ain’t been on my mind',\n",
              "        'และฉันแกล้งทำเป็นว่าไม่มีเธออยู่ในใจ'],\n",
              "       ...,\n",
              "       ['i see the way you vibin’, keep me hypnotized and',\n",
              "        'ฉันเห็นท่าทางของเธอ ที่คอยสะกดจิตฉันและ'],\n",
              "       ['stay cool, it’s just a kiss', 'ใจเย็นน่า มันก็แค่จูบ จุ๊บ ๆ'],\n",
              "       ['hair toss, check my nails', 'สะบัดหัว เช็คดูเล็บ']], dtype=object)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ds_df = pd.read_csv('dataset/translate_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([['oh, even the white kids!', 'โอ้ แม้แต่เจ้าเด็กผิวขาวก็ด้วย'],\n",
              "       ['lay on the floor of your living room',\n",
              "        'มาลงจอดบนพื้นห้องนั่งเล่นของเธอ'],\n",
              "       ['and i’m pretendin’ you ain’t been on my mind',\n",
              "        'และฉันแกล้งทำเป็นว่าไม่มีเธออยู่ในใจ'],\n",
              "       ['‘cause i warned you, ooh, ooh, ooh, ooh',\n",
              "        'เพราะฉันเตือนเธอแล้ว'],\n",
              "       ['so many things to say if you stay',\n",
              "        'ฉันมีหลายสิ่งจะพูดไป ถ้าเธอยังอยู่ต่อนะ'],\n",
              "       ['you never told me', 'เธอไม่เคยเอ่ยบอกกัน'],\n",
              "       ['you’re not alone like you think you are',\n",
              "        'คุณไม่ได้โดดเดี่ยวเพียงคนเดียวอย่างที่คุณคิด'],\n",
              "       ['lies on lies on lies and every single time you call, i fall for this',\n",
              "        'โกหกซ้ำไปซ้อนมา และทุกครั้งที่เธอโทรหากัน ฉันก็ใจอ่อนทุกครั้ง'],\n",
              "       ['but everyday seems like saturday with you',\n",
              "        'แต่ไม่ว่าวันไหนก็เหมือนวันเสาร์ เมื่อฉันได้อยู่กับเธอ'],\n",
              "       ['bet your house and i’ll call it',\n",
              "        'วางเดิมพันทุกอย่างเลยสิ เดี๋ยวผมจะชนะเดิมพันนั้นเองแหละ']],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ds_df.values[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOaabI8cTwytxNXRQthDh5x",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
